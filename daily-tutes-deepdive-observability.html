<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Dive: LLM Observability & Traceability</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
            line-height: 1.8;
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
            background: #f8f9fa;
            color: #2c3e50;
        }
        .container {
            background: white;
            padding: 50px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        h1 {
            color: #1a1a1a;
            font-size: 2.5em;
            border-bottom: 4px solid #3498db;
            padding-bottom: 15px;
            margin-bottom: 30px;
        }
        h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-top: 50px;
            padding-left: 15px;
            border-left: 6px solid #3498db;
        }
        h3 {
            color: #34495e;
            font-size: 1.5em;
            margin-top: 30px;
        }
        h4 {
            color: #7f8c8d;
            font-size: 1.2em;
            margin-top: 20px;
        }
        .toc {
            background: #ecf0f1;
            padding: 30px;
            border-radius: 8px;
            margin: 30px 0;
        }
        .toc h3 {
            margin-top: 0;
            color: #2c3e50;
        }
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        .toc li {
            padding: 8px 0;
            border-bottom: 1px solid #bdc3c7;
        }
        .toc a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
        }
        .toc a:hover {
            text-decoration: underline;
        }
        .info-box {
            background: #e8f4f8;
            border-left: 5px solid #3498db;
            padding: 20px;
            margin: 25px 0;
            border-radius: 5px;
        }
        .warning-box {
            background: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 20px;
            margin: 25px 0;
            border-radius: 5px;
        }
        .success-box {
            background: #d4edda;
            border-left: 5px solid #28a745;
            padding: 20px;
            margin: 25px 0;
            border-radius: 5px;
        }
        .code-block {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }
        .code-block .comment {
            color: #5c6370;
        }
        .code-block .keyword {
            color: #c678dd;
        }
        .code-block .string {
            color: #98c379;
        }
        .code-block .function {
            color: #61afef;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 2px 3px rgba(0,0,0,0.1);
        }
        th {
            background: #3498db;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        td {
            padding: 12px 15px;
            border-bottom: 1px solid #ddd;
        }
        tr:hover {
            background: #f8f9fa;
        }
        .comparison-table th {
            background: #34495e;
        }
        .diagram {
            background: white;
            border: 2px solid #3498db;
            padding: 30px;
            margin: 30px 0;
            border-radius: 8px;
            text-align: center;
            font-family: 'Courier New', monospace;
        }
        .diagram pre {
            text-align: left;
            margin: 0;
            line-height: 1.6;
        }
        .badge {
            display: inline-block;
            padding: 4px 10px;
            border-radius: 4px;
            font-size: 0.85em;
            font-weight: bold;
            margin-right: 8px;
        }
        .badge-free { background: #28a745; color: white; }
        .badge-paid { background: #e74c3c; color: white; }
        .badge-recommended { background: #f39c12; color: white; }
        .badge-new { background: #9b59b6; color: white; }
        ul, ol {
            margin: 15px 0;
            padding-left: 30px;
        }
        li {
            margin: 8px 0;
        }
        .highlight {
            background: #fff3cd;
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: 600;
        }
        .section-number {
            color: #3498db;
            font-weight: bold;
            margin-right: 10px;
        }
        .analogy-box {
            background: #f0f8ff;
            border: 2px dashed #3498db;
            padding: 25px;
            margin: 25px 0;
            border-radius: 8px;
        }
        .analogy-box h4 {
            color: #3498db;
            margin-top: 0;
        }
        blockquote {
            border-left: 4px solid #3498db;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }
        .timestamp {
            color: #7f8c8d;
            font-style: italic;
            text-align: right;
            margin-top: 20px;
        }
        code {
            background: #f4f4f4;
            padding: 3px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #e74c3c;
        }
        .footer {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 3px solid #ecf0f1;
            text-align: center;
            color: #7f8c8d;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Title and Metadata -->
        <h1>ğŸ” Deep Dive: LLM Observability & Traceability</h1>
        <p style="font-size: 1.2em; color: #7f8c8d;"><em>A Comprehensive Guide to Monitoring, Debugging, and Optimizing AI Applications</em></p>
        <p class="timestamp">Published: December 16, 2025 | Reading Time: ~45 minutes</p>

        <!-- Table of Contents -->
        <div class="toc">
            <h3>ğŸ“‘ Table of Contents</h3>
            <ul>
                <li><a href="#introduction" onclick="event.preventDefault(); document.getElementById('introduction').scrollIntoView({behavior: 'smooth'});">1. Introduction to LLM Observability</a></li>
                <li><a href="#why-observability" onclick="event.preventDefault(); document.getElementById('why-observability').scrollIntoView({behavior: 'smooth'});">2. Why Observability Matters for AI Applications</a></li>
                <li><a href="#opentelemetry" onclick="event.preventDefault(); document.getElementById('opentelemetry').scrollIntoView({behavior: 'smooth'});">3. OpenTelemetry: The Foundation</a></li>
                <li><a href="#arize-phoenix" onclick="event.preventDefault(); document.getElementById('arize-phoenix').scrollIntoView({behavior: 'smooth'});">4. Arize Phoenix: Deep Evaluation & Analysis</a></li>
                <li><a href="#helicone" onclick="event.preventDefault(); document.getElementById('helicone').scrollIntoView({behavior: 'smooth'});">5. Helicone: Operational Excellence</a></li>
                <li><a href="#antigravity" onclick="event.preventDefault(); document.getElementById('antigravity').scrollIntoView({behavior: 'smooth'});">6. Antigravity's Observability Stack</a></li>
                <li><a href="#integration" onclick="event.preventDefault(); document.getElementById('integration').scrollIntoView({behavior: 'smooth'});">7. Integrating Arize & Helicone with Antigravity</a></li>
                <li><a href="#laymen" onclick="event.preventDefault(); document.getElementById('laymen').scrollIntoView({behavior: 'smooth'});">8. Understanding for Non-Technical Stakeholders</a></li>
                <li><a href="#best-practices" onclick="event.preventDefault(); document.getElementById('best-practices').scrollIntoView({behavior: 'smooth'});">9. Best Practices & Recommendations</a></li>
                <li><a href="#conclusion" onclick="event.preventDefault(); document.getElementById('conclusion').scrollIntoView({behavior: 'smooth'});">10. Conclusion & Next Steps</a></li>
            </ul>
        </div>

        <!-- Section 1: Introduction -->
        <h2 id="introduction"><span class="section-number">1.</span> Introduction to LLM Observability</h2>
        
        <p>As organizations increasingly deploy Large Language Models (LLMs) in production environments, the need for robust observability and traceability becomes critical. Unlike traditional software systems, LLMs introduce unique challenges:</p>
        
        <ul>
            <li><strong>Non-deterministic behavior:</strong> Same input can produce different outputs</li>
            <li><strong>Complex reasoning chains:</strong> Multi-step processes involving retrieval, reasoning, and generation</li>
            <li><strong>Cost variability:</strong> Token-based pricing creates unpredictable expenses</li>
            <li><strong>Quality assessment:</strong> Subjective evaluation of "good" vs "bad" responses</li>
            <li><strong>Hallucinations:</strong> Models generating plausible but incorrect information</li>
        </ul>

        <div class="info-box">
            <h4>ğŸ’¡ Key Insight</h4>
            <p><strong>Observability is not just monitoring.</strong> While monitoring tells you "something is wrong," observability helps you understand "why it's wrong" and "how to fix it." For LLM applications, this distinction is critical.</p>
        </div>

        <h3>The Three Pillars of Observability</h3>
        
        <p>Traditional observability relies on three core data types:</p>
        
        <ol>
            <li><strong>Traces:</strong> Track the journey of a request through your system</li>
            <li><strong>Metrics:</strong> Numeric measurements over time (latency, cost, usage)</li>
            <li><strong>Logs:</strong> Timestamped event records</li>
        </ol>

        <p>For LLM applications, we add additional dimensions:</p>
        
        <ul>
            <li><strong>Prompts & Responses:</strong> The actual input/output pairs</li>
            <li><strong>Evaluations:</strong> Quality scores, hallucination detection, toxicity checks</li>
            <li><strong>Embeddings:</strong> Vector representations for semantic analysis</li>
            <li><strong>Retrieval Context:</strong> Documents and data used in RAG systems</li>
        </ul>

        <!-- Section 2: Why Observability Matters -->
        <h2 id="why-observability"><span class="section-number">2.</span> Why Observability Matters for AI Applications</h2>

        <h3>Real-World Challenges</h3>

        <div class="warning-box">
            <h4>âš ï¸ Without Observability</h4>
            <p>You're essentially flying blind. Here's what can go wrong:</p>
            <ul>
                <li>A single user asks the same question 1,000 times, costing you $100 before you notice</li>
                <li>Your AI starts hallucinating in 30% of responses, but users just stop using your app</li>
                <li>Response quality degrades 40% after a model update, but you don't catch it for weeks</li>
                <li>Your RAG system retrieves irrelevant documents 60% of the time</li>
                <li>A malicious user discovers prompt injection vulnerabilities</li>
            </ul>
        </div>

        <h3>The Cost of Poor Observability</h3>

        <table>
            <tr>
                <th>Problem</th>
                <th>Without Observability</th>
                <th>With Observability</th>
            </tr>
            <tr>
                <td><strong>Runaway Costs</strong></td>
                <td>Discover $10K bill at month-end</td>
                <td>Real-time alerts at $100/day threshold</td>
            </tr>
            <tr>
                <td><strong>Quality Degradation</strong></td>
                <td>Users complain after 2 weeks</td>
                <td>Detect within hours via automated evals</td>
            </tr>
            <tr>
                <td><strong>Debug Time</strong></td>
                <td>Days to reproduce and fix</td>
                <td>Minutes to identify root cause</td>
            </tr>
            <tr>
                <td><strong>User Churn</strong></td>
                <td>Lost users before you know why</td>
                <td>Proactive improvements based on data</td>
            </tr>
        </table>

        <h3>Business Value of Observability</h3>

        <div class="success-box">
            <h4>âœ… Proven ROI</h4>
            <ul>
                <li><strong>Cost Optimization:</strong> Teams report 20-40% cost reduction through caching and optimization</li>
                <li><strong>Quality Improvement:</strong> 50-80% faster iteration on prompt engineering</li>
                <li><strong>Faster Debugging:</strong> 10x reduction in time to identify issues</li>
                <li><strong>Compliance:</strong> Audit trails for AI governance and regulatory requirements</li>
                <li><strong>User Trust:</strong> Demonstrate safety and reliability to stakeholders</li>
            </ul>
        </div>

        <!-- Section 3: OpenTelemetry -->
        <h2 id="opentelemetry"><span class="section-number">3.</span> OpenTelemetry: The Foundation</h2>

        <h3>What is OpenTelemetry?</h3>

        <p>OpenTelemetry (OTel) is the open-source industry standard for collecting, processing, and exporting telemetry data. Think of it as the "HTTP of observability" - a universal protocol that everyone speaks.</p>

        <div class="info-box">
            <h4>ğŸ¯ Core Principle</h4>
            <p><strong>Instrument Once, Send Anywhere:</strong> Write your instrumentation code once using OpenTelemetry, then send that data to any compatible backend (Phoenix, Datadog, Grafana, etc.) without changing your code.</p>
        </div>

        <h3>Why OpenTelemetry Matters</h3>

        <h4>The Problem Before OTel</h4>
        <ul>
            <li>Each monitoring vendor had proprietary instrumentation</li>
            <li>Switching vendors meant rewriting all instrumentation code</li>
            <li>Lock-in to specific vendors</li>
            <li>Fragmented ecosystem with incompatible tools</li>
        </ul>

        <h4>The Solution With OTel</h4>
        <ul>
            <li><strong>Vendor Neutrality:</strong> Your data, your choice of backend</li>
            <li><strong>Standardization:</strong> Common language across teams and tools</li>
            <li><strong>Future-Proof:</strong> Investments in instrumentation remain valuable</li>
            <li><strong>Ecosystem:</strong> 90+ supported vendors and backends</li>
        </ul>

        <h3>OpenTelemetry Architecture</h3>

        <div class="diagram">
            <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         YOUR APPLICATION                        â”‚
â”‚  (Instrumented with OpenTelemetry SDK)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      OpenTelemetry Collector (Optional)         â”‚
â”‚  â€¢ Receives telemetry data                      â”‚
â”‚  â€¢ Processes (filters, batches, enriches)       â”‚
â”‚  â€¢ Exports to multiple backends                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Phoenix â”‚  â”‚Datadog  â”‚  â”‚ Grafana  â”‚
    â”‚  (OSS) â”‚  â”‚ (Paid)  â”‚  â”‚  (OSS)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            </pre>
        </div>

        <h3>Key Components</h3>

        <table>
            <tr>
                <th>Component</th>
                <th>Purpose</th>
                <th>Example</th>
            </tr>
            <tr>
                <td><strong>API</strong></td>
                <td>Define how to create spans, metrics, logs</td>
                <td><code>trace.getActiveSpan()</code></td>
            </tr>
            <tr>
                <td><strong>SDK</strong></td>
                <td>Implement the API for each language</td>
                <td>Python SDK, JavaScript SDK</td>
            </tr>
            <tr>
                <td><strong>Instrumentation</strong></td>
                <td>Auto-instrument popular frameworks</td>
                <td>OpenAI, LangChain, FastAPI</td>
            </tr>
            <tr>
                <td><strong>Collector</strong></td>
                <td>Receive, process, and export telemetry</td>
                <td>OpenTelemetry Collector</td>
            </tr>
            <tr>
                <td><strong>OTLP</strong></td>
                <td>Protocol for transmitting data</td>
                <td>gRPC or HTTP transport</td>
            </tr>
        </table>

        <h3>OpenTelemetry for LLMs: OpenInference</h3>

        <p>While OpenTelemetry handles general telemetry, <strong>OpenInference</strong> extends it with LLM-specific conventions:</p>

        <ul>
            <li>Semantic conventions for LLM operations (prompts, completions, embeddings)</li>
            <li>Specialized span attributes for AI metadata</li>
            <li>Support for multi-modal data (text, images, audio)</li>
            <li>RAG-specific tracing (retrieval, reranking, generation)</li>
        </ul>

        <div class="code-block">
<span class="comment"># Example: OpenInference with LangChain</span>
<span class="keyword">from</span> openinference.instrumentation.langchain <span class="keyword">import</span> LangChainInstrumentor
<span class="keyword">from</span> phoenix.otel <span class="keyword">import</span> register

<span class="comment"># Setup Phoenix as the backend</span>
tracer_provider = <span class="function">register</span>(
    endpoint=<span class="string">"http://localhost:6006/v1/traces"</span>,
    project_name=<span class="string">"my-llm-app"</span>
)

<span class="comment"># Auto-instrument LangChain</span>
LangChainInstrumentor().<span class="function">instrument</span>(tracer_provider=tracer_provider)

<span class="comment"># Now all LangChain calls are automatically traced!</span>
        </div>

        <h3>Status in 2025</h3>

        <div class="success-box">
            <h4>âœ… Production-Ready</h4>
            <ul>
                <li><strong>Traces:</strong> Stable since 2021</li>
                <li><strong>Metrics:</strong> Stable since 2021</li>
                <li><strong>Logs:</strong> Stable since late 2023</li>
                <li><strong>Adoption:</strong> 2nd most active CNCF project (after Kubernetes)</li>
                <li><strong>Support:</strong> Microsoft, Google, Amazon, Datadog, Splunk, Arize, and 85+ others</li>
            </ul>
        </div>

        <!-- Section 4: Arize Phoenix -->
        <h2 id="arize-phoenix"><span class="section-number">4.</span> Arize Phoenix: Deep Evaluation & Analysis</h2>

        <h3>Overview</h3>

        <p><strong>Arize Phoenix</strong> is an open-source LLM observability platform built on OpenTelemetry that specializes in <span class="highlight">evaluation, quality assessment, and deep analysis</span> of AI applications.</p>

        <div class="info-box">
            <h4>ğŸ¯ Phoenix Philosophy</h4>
            <p>Phoenix answers the question: <em>"Is my AI actually good?"</em> It goes beyond basic monitoring to provide sophisticated evaluation capabilities that help you understand and improve AI quality.</p>
        </div>

        <h3>Key Features</h3>

        <h4>1. Tracing & Observability</h4>
        <ul>
            <li>Full distributed tracing for multi-step LLM workflows</li>
            <li>Automatic instrumentation for popular frameworks (LangChain, LlamaIndex, DSPy)</li>
            <li>Support for all major LLM providers (OpenAI, Anthropic, Bedrock, VertexAI)</li>
            <li>Trace visualization with interactive timeline views</li>
        </ul>

        <h4>2. Evaluations (â˜… Best-in-Class)</h4>
        <p>Phoenix provides pre-built evaluators for common AI quality metrics:</p>

        <table>
            <tr>
                <th>Evaluation Type</th>
                <th>What It Checks</th>
                <th>Use Case</th>
            </tr>
            <tr>
                <td><strong>Hallucination Detection</strong></td>
                <td>Does the response contain factually incorrect information?</td>
                <td>RAG systems, factual QA</td>
            </tr>
            <tr>
                <td><strong>Relevance</strong></td>
                <td>Is the response relevant to the query?</td>
                <td>Search, chatbots</td>
            </tr>
            <tr>
                <td><strong>Toxicity</strong></td>
                <td>Does the response contain harmful content?</td>
                <td>User-facing apps</td>
            </tr>
            <tr>
                <td><strong>QA Correctness</strong></td>
                <td>Is the answer factually correct?</td>
                <td>Question answering</td>
            </tr>
            <tr>
                <td><strong>Retrieval Quality</strong></td>
                <td>Are retrieved documents relevant?</td>
                <td>RAG systems</td>
            </tr>
        </table>

        <h4>3. RAG Debugging</h4>
        <p>Phoenix excels at debugging Retrieval-Augmented Generation systems:</p>
        <ul>
            <li><strong>Embedding Visualization:</strong> UMAP/t-SNE plots to understand document clusters</li>
            <li><strong>Retrieval Analysis:</strong> See which documents were retrieved and why</li>
            <li><strong>Drift Detection:</strong> Identify when your embeddings or retrievals change</li>
            <li><strong>Cluster Analysis:</strong> Find patterns in user queries and system responses</li>
        </ul>

        <h4>4. Drift Detection</h4>
        <p>Phoenix monitors for data and model drift over time:</p>
        <ul>
            <li>Statistical drift detection (PSI, KL divergence)</li>
            <li>Visual drift monitoring with intuitive dashboards</li>
            <li>Alerts when drift exceeds thresholds</li>
            <li>Historical comparisons to baseline periods</li>
        </ul>

        <h3>Phoenix Architecture</h3>

        <p>Phoenix offers two products:</p>

        <h4>Phoenix (Open Source) <span class="badge badge-free">FREE</span></h4>
        <ul>
            <li><strong>License:</strong> Elastic License 2.0 (ELv2)</li>
            <li><strong>Deployment:</strong> Self-hosted (local, Docker, Kubernetes)</li>
            <li><strong>Features:</strong> All core features included</li>
            <li><strong>Cost:</strong> Completely free, no feature gates</li>
            <li><strong>Cloud Option:</strong> Free tier with 1K logs/month</li>
        </ul>

        <h4>Arize AX (Enterprise) <span class="badge badge-paid">PAID</span></h4>
        <ul>
            <li><strong>Pricing:</strong> $50K+/year (custom pricing)</li>
            <li><strong>Deployment:</strong> Fully managed cloud</li>
            <li><strong>Additional Features:</strong> RBAC, Alyx AI assistant, advanced Prompt IDE, professional support</li>
            <li><strong>Best For:</strong> Large enterprises with production-critical AI systems</li>
        </ul>

        <h3>Integration Time</h3>

        <div class="warning-box">
            <h4>â±ï¸ Time Investment</h4>
            <p><strong>Initial Setup:</strong> 2-4 hours for full integration</p>
            <ul>
                <li>30 minutes: Install and launch Phoenix</li>
                <li>1-2 hours: Instrument your application</li>
                <li>1-2 hours: Configure evaluations and dashboards</li>
            </ul>
        </div>

        <h3>Code Example: Phoenix Integration</h3>

        <div class="code-block">
<span class="comment"># Install Phoenix</span>
pip install arize-phoenix opentelemetry-api opentelemetry-sdk

<span class="comment"># Launch Phoenix server (runs locally on port 6006)</span>
python -m phoenix.server.main serve

<span class="comment"># In your application code:</span>
<span class="keyword">from</span> phoenix.otel <span class="keyword">import</span> register
<span class="keyword">from</span> openinference.instrumentation.openai <span class="keyword">import</span> OpenAIInstrumentor

<span class="comment"># Setup Phoenix tracing</span>
tracer_provider = <span class="function">register</span>(
    endpoint=<span class="string">"http://localhost:6006/v1/traces"</span>,
    project_name=<span class="string">"my-ai-app"</span>
)

<span class="comment"># Auto-instrument OpenAI</span>
OpenAIInstrumentor().<span class="function">instrument</span>(tracer_provider=tracer_provider)

<span class="comment"># Now make OpenAI calls normally - they're automatically traced!</span>
<span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI
client = <span class="function">OpenAI</span>()
response = client.chat.completions.<span class="function">create</span>(
    model=<span class="string">"gpt-4"</span>,
    messages=[{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"Hello!"</span>}]
)

<span class="comment"># View traces at: http://localhost:6006</span>
        </div>

        <h3>When to Choose Phoenix</h3>

        <div class="success-box">
            <h4>âœ… Choose Phoenix When You Need:</h4>
            <ul>
                <li><strong>Deep Evaluations:</strong> Best-in-class eval capabilities</li>
                <li><strong>RAG Debugging:</strong> Embedding visualization and retrieval analysis</li>
                <li><strong>Drift Detection:</strong> Monitor for data/model drift over time</li>
                <li><strong>Open Source:</strong> Full control, self-hostable, no vendor lock-in</li>
                <li><strong>Research & Development:</strong> Experiment with prompts and models</li>
            </ul>
        </div>

        <!-- Section 5: Helicone -->
        <h2 id="helicone"><span class="section-number">5.</span> Helicone: Operational Excellence</h2>

        <h3>Overview</h3>

        <p><strong>Helicone</strong> is an open-source LLM observability platform that takes a fundamentally different approach: <span class="highlight">proxy-based integration</span> for operational metrics and cost optimization.</p>

        <div class="info-box">
            <h4>ğŸ¯ Helicone Philosophy</h4>
            <p>Helicone answers the question: <em>"How much am I spending and can I optimize it?"</em> It focuses on operational concerns: cost tracking, caching, rate limiting, and performance.</p>
        </div>

        <h3>The Proxy Approach</h3>

        <p>Instead of instrumenting your code, Helicone works as a transparent proxy:</p>

        <div class="diagram">
            <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your App       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (Just change baseURL)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Helicone Proxy                    â”‚
â”‚  â€¢ Logs all requests/responses          â”‚
â”‚  â€¢ Tracks costs automatically           â”‚
â”‚  â€¢ Caches duplicate requests            â”‚
â”‚  â€¢ Enforces rate limits                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI/Anthropicâ”‚
â”‚   /Gemini APIs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            </pre>
        </div>

        <h3>Key Features</h3>

        <h4>1. One-Line Integration âš¡</h4>
        <p>The fastest integration of any observability tool:</p>

        <div class="code-block">
<span class="comment">// Before (direct OpenAI call)</span>
<span class="keyword">const</span> openai = <span class="keyword">new</span> <span class="function">OpenAI</span>({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: <span class="string">"https://api.openai.com/v1"</span>
});

<span class="comment">// After (via Helicone - only 2 lines changed!)</span>
<span class="keyword">const</span> openai = <span class="keyword">new</span> <span class="function">OpenAI</span>({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: <span class="string">"https://oai.helicone.ai/v1"</span>,  <span class="comment">// â† Change 1</span>
  defaultHeaders: {
    <span class="string">"Helicone-Auth"</span>: `Bearer ${process.env.HELICONE_API_KEY}` <span class="comment">// â† Change 2</span>
  }
});

<span class="comment">// That's it! Now every call is logged, tracked, and optimized</span>
        </div>

        <h4>2. Built-in Caching ğŸ’°</h4>
        <p>Helicone can cache responses to save 20-40% on costs:</p>

        <div class="code-block">
defaultHeaders: {
  <span class="string">"Helicone-Auth"</span>: `Bearer ${process.env.HELICONE_API_KEY}`,
  <span class="string">"Helicone-Cache-Enabled"</span>: <span class="string">"true"</span>,
  <span class="string">"Cache-Control"</span>: <span class="string">"max-age=3600"</span>  <span class="comment">// Cache for 1 hour</span>
}

<span class="comment">// Now duplicate requests within 1 hour return instantly at 0 cost!</span>
        </div>

        <h4>3. Cost Tracking & Analytics ğŸ“Š</h4>
        <p>Automatic cost calculation with granular breakdowns:</p>
        <ul>
            <li>Cost per request, user, session, or time period</li>
            <li>Identify your most expensive users/features</li>
            <li>Track trends over time</li>
            <li>Budget alerts and forecasting</li>
        </ul>

        <h4>4. Rate Limiting ğŸš¦</h4>
        <p>Protect against runaway costs:</p>

        <div class="code-block">
<span class="string">"Helicone-RateLimit-Policy"</span>: <span class="string">"100;w=3600;s=user"</span>
<span class="comment">// Limit: 100 requests per hour per user</span>
        </div>

        <h4>5. User & Session Tracking ğŸ‘¥</h4>
        <p>Track metrics at the user and session level:</p>

        <div class="code-block">
defaultHeaders: {
  <span class="string">"Helicone-Auth"</span>: `Bearer ${process.env.HELICONE_API_KEY}`,
  <span class="string">"Helicone-User-Id"</span>: <span class="string">"user-abc-123"</span>,
  <span class="string">"Helicone-Session-Id"</span>: <span class="string">"session-xyz-789"</span>,
  <span class="string">"Helicone-Session-Path"</span>: <span class="string">"main/research/analysis"</span>
}
        </div>

        <h4>6. Gateway Features ğŸ”€</h4>
        <p>Advanced routing and reliability:</p>
        <ul>
            <li><strong>Load Balancing:</strong> Distribute requests across multiple API keys</li>
            <li><strong>Failover:</strong> Automatic retry with fallback providers</li>
            <li><strong>Routing:</strong> Route to different models based on criteria</li>
            <li><strong>A/B Testing:</strong> Split traffic between model versions</li>
        </ul>

        <h4>7. Security: Prompt Armor ğŸ›¡ï¸</h4>
        <p>Built-in protection against prompt injection and adversarial attacks.</p>

        <h3>Performance</h3>

        <div class="success-box">
            <h4>âš¡ Ultra-Low Latency</h4>
            <ul>
                <li><strong>Built in Rust:</strong> High-performance, compiled language</li>
                <li><strong>Cloudflare Workers:</strong> Deployed globally at the edge</li>
                <li><strong>P99 Latency:</strong> Less than 1ms overhead</li>
                <li><strong>P50 Latency:</strong> Often sub-millisecond</li>
            </ul>
        </div>

        <h3>Deployment Options</h3>

        <h4>Helicone Cloud <span class="badge badge-recommended">RECOMMENDED</span></h4>
        <ul>
            <li><strong>Free Tier:</strong> 100K requests/month (very generous!)</li>
            <li><strong>Pro:</strong> $20/month</li>
            <li><strong>Team:</strong> $200/month</li>
            <li><strong>Infrastructure:</strong> Cloudflare Workers (globally distributed)</li>
            <li><strong>Setup:</strong> 5-15 minutes</li>
        </ul>

        <h4>Self-Hosted <span class="badge badge-free">FREE</span></h4>
        <ul>
            <li><strong>License:</strong> Apache 2.0 (very permissive)</li>
            <li><strong>Deployment:</strong> Docker or Kubernetes</li>
            <li><strong>Requirements:</strong> PostgreSQL, ClickHouse, Kafka</li>
            <li><strong>Best For:</strong> Data sovereignty requirements</li>
        </ul>

        <h3>Helicone vs Arize Phoenix</h3>

        <table class="comparison-table">
            <tr>
                <th>Feature</th>
                <th>Helicone</th>
                <th>Arize Phoenix</th>
            </tr>
            <tr>
                <td><strong>Integration Method</strong></td>
                <td>Proxy (1 line)</td>
                <td>OpenTelemetry (code instrumentation)</td>
            </tr>
            <tr>
                <td><strong>Setup Time</strong></td>
                <td>15 minutes</td>
                <td>2-4 hours</td>
            </tr>
            <tr>
                <td><strong>Free Tier</strong></td>
                <td>100K requests/month</td>
                <td>1K logs/month (cloud) or unlimited (self-hosted)</td>
            </tr>
            <tr>
                <td><strong>Cost Tracking</strong></td>
                <td>â­â­â­ Excellent</td>
                <td>â­ Basic</td>
            </tr>
            <tr>
                <td><strong>Caching</strong></td>
                <td>âœ… Built-in</td>
                <td>âŒ No</td>
            </tr>
            <tr>
                <td><strong>Rate Limiting</strong></td>
                <td>âœ… Built-in</td>
                <td>âŒ No</td>
            </tr>
            <tr>
                <td><strong>Evaluations</strong></td>
                <td>â­ Basic</td>
                <td>â­â­â­ Best-in-class</td>
            </tr>
            <tr>
                <td><strong>Drift Detection</strong></td>
                <td>âŒ No</td>
                <td>â­â­â­ Excellent</td>
            </tr>
            <tr>
                <td><strong>RAG Debugging</strong></td>
                <td>â­ Basic</td>
                <td>â­â­â­ Excellent</td>
            </tr>
            <tr>
                <td><strong>Embedding Visualization</strong></td>
                <td>âŒ No</td>
                <td>âœ… Yes</td>
            </tr>
            <tr>
                <td><strong>Best Use Case</strong></td>
                <td>Operations, cost optimization</td>
                <td>Quality, evaluations, research</td>
            </tr>
        </table>

        <h3>When to Choose Helicone</h3>

        <div class="success-box">
            <h4>âœ… Choose Helicone When You Need:</h4>
            <ul>
                <li><strong>Fast Integration:</strong> Live in 15 minutes</li>
                <li><strong>Cost Optimization:</strong> Track and reduce LLM expenses</li>
                <li><strong>Caching:</strong> Save 20-40% with intelligent caching</li>
                <li><strong>Rate Limiting:</strong> Protect against abuse and runaway costs</li>
                <li><strong>Operational Metrics:</strong> Latency, errors, usage patterns</li>
                <li><strong>User-Level Tracking:</strong> Per-user/session analytics</li>
            </ul>
        </div>

        <!-- Section 6: Antigravity -->
        <h2 id="antigravity"><span class="section-number">6.</span> Antigravity's Observability Stack</h2>

        <h3>Overview</h3>

        <p>Google Antigravity (launched November 2025) is an AI-first development platform with built-in observability capabilities. As a Google product, it integrates deeply with the Google Cloud ecosystem.</p>

        <h3>Core Observability Technology</h3>

        <p>Antigravity uses <strong>OpenTelemetry</strong> as its foundation:</p>

        <ul>
            <li><strong>Protocol:</strong> OTLP (OpenTelemetry Protocol)</li>
            <li><strong>Export:</strong> gRPC or HTTP</li>
            <li><strong>Semantic Conventions:</strong> OpenInference for LLM-specific attributes</li>
        </ul>

        <h3>Built-in Integration Points</h3>

        <h4>1. Google Cloud Trace</h4>
        <ul>
            <li>Native OTLP support via <code>telemetry.googleapis.com</code></li>
            <li>Stores traces in OpenTelemetry data model natively</li>
            <li>Enhanced limits: 512-byte attribute keys, 64KB values, 1024 attributes per span</li>
        </ul>

        <h4>2. Google Cloud Monitoring</h4>
        <ul>
            <li>Collects Prometheus-style metrics</li>
            <li>Integrates with OpenTelemetry Collector</li>
            <li>Self-observability metrics for the platform itself</li>
        </ul>

        <h4>3. Google Cloud Logging</h4>
        <ul>
            <li>Structured log ingestion via OpenTelemetry</li>
            <li>Correlation with traces via trace IDs</li>
            <li>Logs Explorer for search and analysis</li>
        </ul>

        <h3>Pre-Built Workflows</h3>

        <p>Antigravity includes observability workflows out-of-the-box:</p>

        <table>
            <tr>
                <th>Workflow</th>
                <th>Purpose</th>
                <th>Technology</th>
            </tr>
            <tr>
                <td><strong>Trace Distributed Requests</strong></td>
                <td>Track requests across services</td>
                <td>OpenTelemetry + Jaeger</td>
            </tr>
            <tr>
                <td><strong>AgentOps</strong></td>
                <td>AI agent observability</td>
                <td>AgentOps API</td>
            </tr>
            <tr>
                <td><strong>VictoriaMetrics</strong></td>
                <td>Monitoring & metrics</td>
                <td>VictoriaMetrics APIs</td>
            </tr>
            <tr>
                <td><strong>Prometheus + Grafana</strong></td>
                <td>Four golden signals monitoring</td>
                <td>Prometheus, Grafana</td>
            </tr>
        </table>

        <h3>Visualization Options</h3>

        <h4>For Development:</h4>
        <ul>
            <li><strong>Jaeger:</strong> Open-source distributed tracing UI</li>
            <li><strong>Local:</strong> <code>docker run -d -p 16686:16686 jaegertracing/all-in-one</code></li>
        </ul>

        <h4>For Production:</h4>
        <ul>
            <li><strong>Google Cloud Trace:</strong> Fully managed, scalable</li>
            <li><strong>Datadog:</strong> Recommended by Antigravity docs</li>
            <li><strong>Custom backends:</strong> Any OTLP-compatible system</li>
        </ul>

        <h3>Example: Basic Tracing in Antigravity</h3>

        <div class="code-block">
<span class="comment">// Install OpenTelemetry</span>
npm install @opentelemetry/api

<span class="comment">// In your Antigravity code</span>
<span class="keyword">import</span> { trace } <span class="keyword">from</span> <span class="string">'@opentelemetry/api'</span>;

<span class="comment">// Get trace ID for current operation</span>
<span class="keyword">const</span> traceId = trace.<span class="function">getActiveSpan</span>()?.spanContext().traceId;

<span class="comment">// Traces are automatically exported to configured backend</span>
<span class="comment">// View in Jaeger: http://localhost:16686</span>
<span class="comment">// Or in Google Cloud Trace: console.cloud.google.com/traces</span>
        </div>

        <!-- Section 7: Integration -->
        <h2 id="integration"><span class="section-number">7.</span> Integrating Arize & Helicone with Antigravity</h2>

        <h3>The Best of Both Worlds Strategy</h3>

        <p>The optimal approach is to use <strong>both Helicone and Phoenix together</strong>. They complement each other perfectly and don't conflict:</p>

        <div class="diagram">
            <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Antigravity Application              â”‚
â”‚   (Your AI-powered development tool)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                                  â”‚
   â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Helicone      â”‚         â”‚  Arize Phoenix   â”‚
â”‚   (Proxy)       â”‚         â”‚  (Telemetry)     â”‚
â”‚                 â”‚         â”‚                  â”‚
â”‚ â€¢ Cost tracking â”‚         â”‚ â€¢ Evaluations    â”‚
â”‚ â€¢ Caching       â”‚         â”‚ â€¢ Quality checks â”‚
â”‚ â€¢ Rate limiting â”‚         â”‚ â€¢ Drift detect   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI/Anthropic â”‚
â”‚  /Gemini APIs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            </pre>
        </div>

        <h3>Why This Works</h3>

        <div class="success-box">
            <h4>âœ… Non-Conflicting Architecture</h4>
            <ul>
                <li><strong>Helicone:</strong> Sits in the request path (proxy) - handles operational concerns</li>
                <li><strong>Phoenix:</strong> Receives telemetry separately (observer) - handles quality concerns</li>
                <li><strong>Independence:</strong> Each works independently, failure of one doesn't affect the other</li>
                <li><strong>Complementary:</strong> Together they provide complete observability</li>
            </ul>
        </div>

        <h3>Step-by-Step Integration</h3>

        <h4>Option 1: Helicone + Phoenix (Recommended)</h4>

        <div class="code-block">
<span class="comment">// Step 1: Install dependencies</span>
npm install openai @opentelemetry/api
pip install arize-phoenix opentelemetry-exporter-otlp

<span class="comment">// Step 2: Launch Phoenix</span>
python -m phoenix.server.main serve
<span class="comment">// Phoenix UI available at: http://localhost:6006</span>

<span class="comment">// Step 3: Configure in your Antigravity code</span>

<span class="keyword">import</span> OpenAI <span class="keyword">from</span> <span class="string">"openai"</span>;
<span class="keyword">import</span> { register } <span class="keyword">from</span> <span class="string">"phoenix-otel"</span>;

<span class="comment">// Setup Phoenix for evaluations and quality monitoring</span>
<span class="keyword">const</span> phoenixTracer = <span class="function">register</span>({
  endpoint: <span class="string">"http://localhost:6006/v1/traces"</span>,
  project_name: <span class="string">"antigravity-app"</span>
});

<span class="comment">// Setup Helicone for operations and cost tracking</span>
<span class="keyword">const</span> openai = <span class="keyword">new</span> <span class="function">OpenAI</span>({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: <span class="string">"https://oai.helicone.ai/v1"</span>,  <span class="comment">// Helicone proxy</span>
  defaultHeaders: {
    <span class="string">"Helicone-Auth"</span>: `Bearer ${process.env.HELICONE_API_KEY}`,
    <span class="string">"Helicone-Cache-Enabled"</span>: <span class="string">"true"</span>,
    <span class="string">"Helicone-User-Id"</span>: <span class="string">"current-user-id"</span>,
  },
});

<span class="comment">// Step 4: Make LLM calls - both systems capture data automatically!</span>
<span class="keyword">const</span> response = <span class="keyword">await</span> openai.chat.completions.<span class="function">create</span>({
  model: <span class="string">"gpt-4o"</span>,
  messages: [{ role: <span class="string">"user"</span>, content: <span class="string">"Generate code..."</span> }]
});

<span class="comment">// Helicone dashboard: Cost, latency, caching stats</span>
<span class="comment">// Phoenix dashboard: Quality scores, hallucination detection</span>
        </div>

        <h4>Option 2: Phoenix Only (Deep Analysis Focus)</h4>

        <div class="code-block">
<span class="comment">// If you only need evaluations and quality monitoring</span>

<span class="keyword">from</span> phoenix.otel <span class="keyword">import</span> register
<span class="keyword">from</span> openinference.instrumentation.openai <span class="keyword">import</span> OpenAIInstrumentor

<span class="comment"># Setup Phoenix</span>
tracer_provider = <span class="function">register</span>(
    endpoint=<span class="string">"http://localhost:6006/v1/traces"</span>,
    project_name=<span class="string">"antigravity"</span>
)

<span class="comment"># Auto-instrument OpenAI calls</span>
OpenAIInstrumentor().<span class="function">instrument</span>(tracer_provider=tracer_provider)

<span class="comment"># Use OpenAI normally - traces go to Phoenix</span>
<span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI
client = <span class="function">OpenAI</span>()
response = client.chat.completions.<span class="function">create</span>(
    model=<span class="string">"gpt-4"</span>,
    messages=[{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"Hello"</span>}]
)
        </div>

        <h4>Option 3: Helicone Only (Operations Focus)</h4>

        <div class="code-block">
<span class="comment">// If you only need cost tracking and operational metrics</span>

<span class="keyword">import</span> OpenAI <span class="keyword">from</span> <span class="string">"openai"</span>;

<span class="keyword">const</span> openai = <span class="keyword">new</span> <span class="function">OpenAI</span>({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: <span class="string">"https://oai.helicone.ai/v1"</span>,
  defaultHeaders: {
    <span class="string">"Helicone-Auth"</span>: `Bearer ${process.env.HELICONE_API_KEY}`,
    <span class="string">"Helicone-Cache-Enabled"</span>: <span class="string">"true"</span>,
    <span class="string">"Cache-Control"</span>: <span class="string">"max-age=3600"</span>,
    <span class="string">"Helicone-RateLimit-Policy"</span>: <span class="string">"100;w=3600;s=user"</span>,
  },
});

<span class="comment">// Done! View dashboard at: helicone.ai</span>
        </div>

        <h3>What You Get with Both</h3>

        <table>
            <tr>
                <th>Question</th>
                <th>Answered By</th>
                <th>Tool</th>
            </tr>
            <tr>
                <td>How much am I spending?</td>
                <td>Real-time cost tracking</td>
                <td><strong>Helicone</strong></td>
            </tr>
            <tr>
                <td>Which users cost the most?</td>
                <td>Per-user analytics</td>
                <td><strong>Helicone</strong></td>
            </tr>
            <tr>
                <td>Can I reduce costs?</td>
                <td>Caching opportunities</td>
                <td><strong>Helicone</strong></td>
            </tr>
            <tr>
                <td>How fast are responses?</td>
                <td>Latency metrics</td>
                <td><strong>Helicone</strong></td>
            </tr>
            <tr>
                <td>Is my AI hallucinating?</td>
                <td>Hallucination detection</td>
                <td><strong>Phoenix</strong></td>
            </tr>
            <tr>
                <td>Are responses high quality?</td>
                <td>Quality evaluations</td>
                <td><strong>Phoenix</strong></td>
            </tr>
            <tr>
                <td>Is quality degrading?</td>
                <td>Drift detection</td>
                <td><strong>Phoenix</strong></td>
            </tr>
            <tr>
                <td>Are my RAG retrievals good?</td>
                <td>Retrieval analysis</td>
                <td><strong>Phoenix</strong></td>
            </tr>
        </table>

        <h3>Complete Integration Example</h3>

        <p>Here's a production-ready setup combining everything:</p>

        <div class="code-block">
<span class="comment">// antigravity-observability-config.js</span>

<span class="keyword">import</span> OpenAI <span class="keyword">from</span> <span class="string">"openai"</span>;
<span class="keyword">import</span> { register } <span class="keyword">from</span> <span class="string">"phoenix-otel"</span>;
<span class="keyword">import</span> { OpenAIInstrumentor } <span class="keyword">from</span> <span class="string">"@arizeai/openinference-instrumentation-openai"</span>;

<span class="comment">// Configuration</span>
<span class="keyword">const</span> config = {
  <span class="comment">// Phoenix for quality monitoring</span>
  phoenix: {
    enabled: <span class="keyword">true</span>,
    endpoint: process.env.PHOENIX_ENDPOINT || <span class="string">"http://localhost:6006/v1/traces"</span>,
    project: <span class="string">"antigravity-production"</span>
  },
  
  <span class="comment">// Helicone for operations</span>
  helicone: {
    enabled: <span class="keyword">true</span>,
    apiKey: process.env.HELICONE_API_KEY,
    caching: <span class="keyword">true</span>,
    cacheTTL: 3600, <span class="comment">// 1 hour</span>
    rateLimit: <span class="string">"100;w=3600;s=user"</span> <span class="comment">// 100 req/hour per user</span>
  }
};

<span class="comment">// Initialize Phoenix</span>
<span class="keyword">if</span> (config.phoenix.enabled) {
  <span class="keyword">const</span> tracer = <span class="function">register</span>({
    endpoint: config.phoenix.endpoint,
    project_name: config.phoenix.project
  });
  
  <span class="comment">// Auto-instrument OpenAI</span>
  OpenAIInstrumentor().<span class="function">instrument</span>({ tracerProvider: tracer });
}

<span class="comment">// Initialize OpenAI with Helicone</span>
<span class="keyword">export</span> <span class="keyword">const</span> openai = <span class="keyword">new</span> <span class="function">OpenAI</span>({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: config.helicone.enabled 
    ? <span class="string">"https://oai.helicone.ai/v1"</span>
    : <span class="string">"https://api.openai.com/v1"</span>,
  defaultHeaders: config.helicone.enabled ? {
    <span class="string">"Helicone-Auth"</span>: `Bearer ${config.helicone.apiKey}`,
    <span class="string">"Helicone-Cache-Enabled"</span>: config.helicone.caching.toString(),
    <span class="string">"Cache-Control"</span>: `max-age=${config.helicone.cacheTTL}`,
    <span class="string">"Helicone-RateLimit-Policy"</span>: config.helicone.rateLimit,
  } : {}
});

<span class="comment">// Helper function to add context to requests</span>
<span class="keyword">export</span> <span class="keyword">function</span> <span class="function">createLLMRequest</span>(userId, sessionId, messages) {
  <span class="keyword">return</span> openai.chat.completions.<span class="function">create</span>({
    model: <span class="string">"gpt-4o"</span>,
    messages,
    <span class="comment">// Helicone metadata (via headers)</span>
    headers: {
      <span class="string">"Helicone-User-Id"</span>: userId,
      <span class="string">"Helicone-Session-Id"</span>: sessionId,
      <span class="string">"Helicone-Property-Environment"</span>: process.env.NODE_ENV
    }
  });
}

<span class="comment">// Usage in your Antigravity code</span>
<span class="keyword">const</span> response = <span class="keyword">await</span> <span class="function">createLLMRequest</span>(
  <span class="string">"user-123"</span>,
  <span class="string">"session-abc"</span>,
  [{ role: <span class="string">"user"</span>, content: <span class="string">"Generate a React component"</span> }]
);

<span class="comment">// Now you have:</span>
<span class="comment">// - Helicone: Cost tracking, caching, rate limiting</span>
<span class="comment">// - Phoenix: Quality scores, hallucination detection</span>
<span class="comment">// - Both: Complete observability!</span>
        </div>

        <!-- Section 8: Layman Explanation -->
        <h2 id="laymen"><span class="section-number">8.</span> Understanding for Non-Technical Stakeholders</h2>

        <div class="analogy-box">
            <h4>ğŸš— The Taxi Service Analogy</h4>
            <p>Let's imagine you're running a taxi company (your AI app) that uses Uber's cars (OpenAI/Anthropic APIs).</p>
        </div>

        <h3>Two Different Problems to Solve</h3>

        <h4>Problem 1: Business Operations ğŸ’°</h4>
        <p><strong>Questions you need answered:</strong></p>
        <ul>
            <li>How much am I spending on gas (API costs)?</li>
            <li>Which routes are most expensive?</li>
            <li>Can I avoid taking the same route twice? (caching)</li>
            <li>Am I driving too much and wasting money? (rate limiting)</li>
            <li>Which customers cost me the most?</li>
        </ul>

        <h4>Problem 2: Service Quality â­</h4>
        <p><strong>Questions you need answered:</strong></p>
        <ul>
            <li>Are my drivers being polite to customers? (AI quality)</li>
            <li>Are they giving wrong directions? (hallucinations)</li>
            <li>Is the service getting worse over time? (drift)</li>
            <li>Are customers satisfied with their rides? (evaluations)</li>
        </ul>

        <h3>The Two Tools</h3>

        <h4>Helicone = Your Business Manager ğŸ’¼</h4>
        
        <p><strong>What it does:</strong></p>
        <ul>
            <li>Sits at the dispatch office</li>
            <li>Tracks every ride's cost</li>
            <li>Remembers frequently-traveled routes (caching)</li>
            <li>Stops you from sending too many cars at once (rate limiting)</li>
            <li>Tells you which customers are costing you money</li>
        </ul>

        <p><strong>How it works:</strong></p>
        <p>Instead of calling Uber directly, you call Helicone first:</p>
        
        <div class="diagram">
            <pre>
You â†’ Helicone (tracks & optimizes) â†’ Uber â†’ Car arrives
            </pre>
        </div>

        <p><strong>What you see in the dashboard:</strong></p>
        <ul>
            <li>"Customer A cost you $50 this month"</li>
            <li>"You asked the same question 10 times - we cached 9 of them and saved $20"</li>
            <li>"Your AI calls are costing $5/day"</li>
            <li>"User X is making 100 calls per hour - maybe slow them down?"</li>
        </ul>

        <h4>Phoenix = Your Quality Inspector ğŸ”</h4>
        
        <p><strong>What it does:</strong></p>
        <ul>
            <li>Rides along in the car (monitors AI behavior)</li>
            <li>Records what the driver says to customers (traces prompts/responses)</li>
            <li>Checks if drivers are polite (evaluates quality)</li>
            <li>Notices if service is getting worse (drift detection)</li>
            <li>Tests if drivers give good directions (evaluations)</li>
        </ul>

        <p><strong>How it works:</strong></p>
        <p>Your app sends "reports" to Phoenix while it's running:</p>
        
        <div class="diagram">
            <pre>
Your App makes AI call
    â†“
OpenTelemetry sends copy of conversation to Phoenix
    â†“
Phoenix analyzes: "Was this response good? Any hallucinations?"
            </pre>
        </div>

        <p><strong>What you see in the dashboard:</strong></p>
        <ul>
            <li>"Your AI hallucinated in 15% of responses today"</li>
            <li>"Response quality dropped 20% this week"</li>
            <li>"This prompt produced better answers than that prompt"</li>
            <li>"Your AI is being toxic in 3% of conversations"</li>
        </ul>

        <h3>Why They Don't Conflict</h3>

        <p>They work at different levels:</p>

        <div class="diagram">
            <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   YOUR ANTIGRAVITY APP                  â”‚
â”‚   (The taxi company)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                          â”‚
   â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Helicone â”‚              â”‚ Phoenix  â”‚
â”‚(Manager)â”‚              â”‚(Quality) â”‚
â”‚  ğŸ’¼     â”‚              â”‚   ğŸ”     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI/Anthropicâ”‚
â”‚   (Uber)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            </pre>
        </div>

        <ul>
            <li><strong>Helicone = On the Road:</strong> Sits between you and the AI provider, like a toll booth</li>
            <li><strong>Phoenix = Watching from Above:</strong> Sits alongside your app, like a GPS tracker</li>
        </ul>

        <h3>Real-World Example</h3>

        <p>Let's say you ask your AI to write an email:</p>

        <h4>With Both Tools:</h4>
        
        <div class="success-box">
            <p><strong>User:</strong> "Write me a professional email"</p>
            <p><strong>â¬‡ï¸ Request goes through Helicone (tracking cost, checking cache)...</strong></p>
            <p><strong>â¬‡ï¸ AI processes and returns email...</strong></p>
            <p><strong>â¬‡ï¸ Phoenix records trace and evaluates quality...</strong></p>
            
            <h4>Helicone Dashboard Shows:</h4>
            <ul>
                <li>Cost: $0.002</li>
                <li>Latency: 1.2 seconds</li>
                <li>User: <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="7e141116103e1b061f130e121b501d1113">[email&#160;protected]</a></li>
                <li>Cached: No (first time)</li>
            </ul>
            
            <h4>Phoenix Dashboard Shows:</h4>
            <ul>
                <li>Quality Score: 8/10</li>
                <li>Hallucination: None detected</li>
                <li>Toxicity: None detected</li>
                <li>Tone: Professional âœ“</li>
            </ul>
        </div>

        <h3>Why Use Both?</h3>

        <div class="analogy-box">
            <h4>ğŸ½ï¸ Running a Restaurant Analogy</h4>
            
            <p><strong>Helicone = Your Accountant:</strong></p>
            <ul>
                <li>Tracks food costs</li>
                <li>Manages suppliers</li>
                <li>Watches for waste</li>
                <li>Controls spending</li>
                <li><strong>â†’ Saves you money</strong></li>
            </ul>
            
            <p><strong>Phoenix = Your Food Critic:</strong></p>
            <ul>
                <li>Tastes the food</li>
                <li>Checks quality</li>
                <li>Rates dishes</li>
                <li>Suggests improvements</li>
                <li><strong>â†’ Improves quality</strong></li>
            </ul>
            
            <p><strong>Would you run a restaurant with ONLY an accountant?</strong> No! Your food might be terrible.</p>
            <p><strong>Would you run a restaurant with ONLY a food critic?</strong> No! You might go bankrupt.</p>
            <p><strong>You need both!</strong></p>
        </div>

        <!-- Section 9: Best Practices -->
        <h2 id="best-practices"><span class="section-number">9.</span> Best Practices & Recommendations</h2>

        <h3>Recommended Implementation Timeline</h3>

        <h4>Week 1: Quick Wins with Helicone</h4>
        
        <div class="success-box">
            <p><strong>Time Investment:</strong> 1-2 hours</p>
            <p><strong>Goal:</strong> Get immediate operational visibility</p>
            
            <p><strong>Tasks:</strong></p>
            <ol>
                <li>Sign up for Helicone (5 minutes)</li>
                <li>Change baseURL in your code (5 minutes)</li>
                <li>Enable caching (2 minutes)</li>
                <li>Set up rate limiting (5 minutes)</li>
                <li>Configure user tracking (15 minutes)</li>
                <li>Explore dashboard and set up alerts (30 minutes)</li>
            </ol>
            
            <p><strong>Immediate Benefits:</strong></p>
            <ul>
                <li>âœ… Know exactly what you're spending</li>
                <li>âœ… Start saving 20-40% with caching</li>
                <li>âœ… Protect against cost overruns</li>
                <li>âœ… Track per-user metrics</li>
            </ul>
        </div>

        <h4>Week 2: Deep Analysis with Phoenix</h4>
        
        <div class="info-box">
            <p><strong>Time Investment:</strong> Half day (4 hours)</p>
            <p><strong>Goal:</strong> Understand AI quality and identify issues</p>
            
            <p><strong>Tasks:</strong></p>
            <ol>
                <li>Install Phoenix locally (15 minutes)</li>
                <li>Configure OpenTelemetry instrumentation (1 hour)</li>
                <li>Run evaluations on existing traces (1 hour)</li>
                <li>Set up dashboards and monitoring (1 hour)</li>
                <li>Configure alerts for quality degradation (30 minutes)</li>
            </ol>
            
            <p><strong>Benefits:</strong></p>
            <ul>
                <li>âœ… Detect hallucinations and quality issues</li>
                <li>âœ… Measure quality over time</li>
                <li>âœ… Optimize prompts based on data</li>
                <li>âœ… Debug RAG systems effectively</li>
            </ul>
        </div>

        <h4>Week 3+: Continuous Optimization</h4>
        
        <ul>
            <li><strong>Daily:</strong> Check Helicone for cost anomalies</li>
            <li><strong>Weekly:</strong> Review Phoenix quality metrics</li>
            <li><strong>Monthly:</strong> Deep dive into optimization opportunities</li>
            <li><strong>Quarterly:</strong> Comprehensive review and strategy adjustment</li>
        </ul>

        <h3>Decision Framework</h3>

        <table>
            <tr>
                <th>Your Situation</th>
                <th>Recommended Approach</th>
            </tr>
            <tr>
                <td>Just getting started</td>
                <td><strong>Start with Helicone</strong><br>Fastest time-to-value, immediate cost insights</td>
            </tr>
            <tr>
                <td>Costs are out of control</td>
                <td><strong>Helicone (urgent)</strong><br>Get visibility and enable caching immediately</td>
            </tr>
            <tr>
                <td>Quality is a concern</td>
                <td><strong>Phoenix first</strong><br>Focus on evaluations and quality metrics</td>
            </tr>
            <tr>
                <td>Production-critical app</td>
                <td><strong>Both together</strong><br>Complete observability is essential</td>
            </tr>
            <tr>
                <td>Research/experimental</td>
                <td><strong>Phoenix (free)</strong><br>Self-host Phoenix for unlimited usage</td>
            </tr>
            <tr>
                <td>Small team, limited time</td>
                <td><strong>Helicone first</strong><br>15-minute setup, immediate value</td>
            </tr>
        </table>

        <h3>Cost Optimization Tips</h3>

        <h4>Using Helicone:</h4>
        <ol>
            <li><strong>Enable caching aggressively:</strong> Start with 1-hour TTL, adjust based on use case</li>
            <li><strong>Set rate limits:</strong> Prevent abuse and runaway costs</li>
            <li><strong>Monitor per-user costs:</strong> Identify and optimize expensive users</li>
            <li><strong>Use async integration:</strong> Avoid Helicone being in critical path</li>
            <li><strong>Review cache hit rates:</strong> Optimize your caching strategy monthly</li>
        </ol>

        <h4>Using Phoenix:</h4>
        <ol>
            <li><strong>Run evaluations on samples:</strong> Don't evaluate every request (expensive)</li>
            <li><strong>Use evaluation triggers:</strong> Evaluate only when quality scores drop</li>
            <li><strong>Optimize prompts:</strong> Use Phoenix data to improve prompt efficiency</li>
            <li><strong>Detect drift early:</strong> Catch quality issues before they become expensive</li>
        </ol>

        <h3>Common Pitfalls to Avoid</h3>

        <div class="warning-box">
            <h4>âš ï¸ Don't Do These Things</h4>
            <ul>
                <li><strong>Don't wait until problems occur:</strong> Set up observability proactively</li>
                <li><strong>Don't ignore the data:</strong> Check dashboards weekly at minimum</li>
                <li><strong>Don't optimize prematurely:</strong> Gather 1-2 weeks of data first</li>
                <li><strong>Don't use only one tool:</strong> Helicone and Phoenix serve different purposes</li>
                <li><strong>Don't expose sensitive data:</strong> Configure PII filtering appropriately</li>
                <li><strong>Don't skip alerting:</strong> Set up alerts for cost and quality thresholds</li>
                <li><strong>Don't instrument everything:</strong> Start with critical paths, expand gradually</li>
            </ul>
        </div>

        <!-- Section 10: Conclusion -->
        <h2 id="conclusion"><span class="section-number">10.</span> Conclusion & Next Steps</h2>

        <h3>Key Takeaways</h3>

        <div class="success-box">
            <h4>ğŸ¯ The Essential Points</h4>
            <ol>
                <li><strong>Observability is non-negotiable</strong> for production LLM applications</li>
                <li><strong>OpenTelemetry is the foundation</strong> - vendor-neutral, industry standard</li>
                <li><strong>Helicone and Phoenix complement each other</strong> - operations vs quality</li>
                <li><strong>They don't conflict</strong> - use both together for complete visibility</li>
                <li><strong>Start with Helicone</strong> for quick wins, add Phoenix for depth</li>
                <li><strong>Antigravity works with both</strong> via OpenTelemetry compatibility</li>
            </ol>
        </div>

        <h3>Your Action Plan</h3>

        <h4>This Week (2 hours):</h4>
        <ol>
            <li>âœ… Sign up for Helicone free tier (5 min)</li>
            <li>âœ… Change baseURL in your Antigravity code (5 min)</li>
            <li>âœ… Enable caching and rate limiting (10 min)</li>
            <li>âœ… Explore Helicone dashboard (1 hour)</li>
            <li>âœ… Set up cost alerts (15 min)</li>
        </ol>

        <h4>Next Week (Half day):</h4>
        <ol>
            <li>âœ… Install Phoenix locally (15 min)</li>
            <li>âœ… Configure OpenTelemetry in Antigravity (1 hour)</li>
            <li>âœ… Run first evaluations (1 hour)</li>
            <li>âœ… Set up quality dashboards (1 hour)</li>
        </ol>

        <h4>Ongoing:</h4>
        <ul>
            <li>Review metrics weekly</li>
            <li>Optimize based on data monthly</li>
            <li>Share insights with team regularly</li>
        </ul>

        <h3>Resources</h3>

        <table>
            <tr>
                <th>Resource</th>
                <th>Link</th>
            </tr>
            <tr>
                <td>Helicone Documentation</td>
                <td><a href="https://docs.helicone.ai/">docs.helicone.ai</a></td>
            </tr>
            <tr>
                <td>Arize Phoenix Docs</td>
                <td><a href="https://docs.arize.com/phoenix">docs.arize.com/phoenix</a></td>
            </tr>
            <tr>
                <td>OpenTelemetry Docs</td>
                <td><a href="https://opentelemetry.io/docs/">opentelemetry.io/docs</a></td>
            </tr>
            <tr>
                <td>Antigravity Docs</td>
                <td><a href="https://antigravity.codes/">antigravity.codes</a></td>
            </tr>
            <tr>
                <td>Google Cloud Trace</td>
                <td><a href="https://cloud.google.com/trace">cloud.google.com/trace</a></td>
            </tr>
        </table>

        <h3>Final Thoughts</h3>

        <blockquote>
            "What gets measured gets managed." - Peter Drucker
        </blockquote>

        <p>In the era of AI applications, this wisdom is more relevant than ever. LLMs are powerful but unpredictable. Without proper observability, you're essentially flying blind - hoping costs stay reasonable, quality remains high, and users stay happy.</p>

        <p><strong>The investment in observability pays for itself quickly:</strong></p>
        <ul>
            <li>Teams report 20-40% cost reduction through optimization</li>
            <li>Quality issues caught in hours instead of weeks</li>
            <li>Debug time reduced by 10x</li>
            <li>User satisfaction improves measurably</li>
        </ul>

        <p>By implementing Helicone and Phoenix with Antigravity, you're not just adding monitoring - you're building a foundation for continuous improvement, cost control, and user trust.</p>

        <div class="success-box">
            <h4>ğŸš€ Ready to Get Started?</h4>
            <p>Don't overthink it. Pick one tool (we recommend Helicone for speed), implement it this week, and expand from there. The perfect setup is the one you actually use.</p>
            <p><strong>Your future self will thank you.</strong></p>
        </div>

        <!-- Footer -->
        <div class="footer">
            <h3>ğŸ“š Deep Dive Complete!</h3>
            <p>You now have a comprehensive understanding of LLM observability and how to implement it with Antigravity.</p>
            <p style="margin-top: 20px;"><strong>Document Version:</strong> 1.0 | <strong>Last Updated:</strong> December 16, 2025</p>
            <p style="margin-top: 10px; font-size: 0.9em;">Continue your learning journey in the Daily Tutes series.</p>
            <p style="margin-top: 10px;"><a href="index.html" style="color: #3498db; text-decoration: none;">â† Back to Daily Tutes Index</a></p>
        </div>
    </div>
</body>
</html>
